{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b51f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ Install and import all libraries\n",
    "!pip install lightfm --quiet\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Please upload all your dataset files (e.g., *.train.csv.gz, *.valid.csv.gz, *.test.csv.gz)\")\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data(file_prefix, min_year=2020):\n",
    "    train_df = pd.read_csv(f'{file_prefix}.train.csv.gz', compression='gzip')\n",
    "    valid_df = pd.read_csv(f'{file_prefix}.valid.csv.gz', compression='gzip')\n",
    "    test_df = pd.read_csv(f'{file_prefix}.test.csv.gz', compression='gzip')\n",
    "\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')\n",
    "\n",
    "    filtered_train_df = train_df[train_df['timestamp'].dt.year >= min_year]\n",
    "    filtered_valid_df = valid_df[valid_df['timestamp'].dt.year >= min_year]\n",
    "    filtered_test_df = test_df[test_df['timestamp'].dt.year >= min_year]\n",
    "\n",
    "    return filtered_train_df, filtered_valid_df, filtered_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc377a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(user_ids, item_ids, ratings, num_users, num_items):\n",
    "    user_input = keras.Input(shape=(1,))\n",
    "    item_input = keras.Input(shape=(1,))\n",
    "    user_embed = layers.Embedding(num_users, 4)(user_input)\n",
    "    item_embed = layers.Embedding(num_items, 4)(item_input)\n",
    "\n",
    "    x = layers.Concatenate()([user_embed, item_embed])\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    history = model.fit([user_ids, item_ids], ratings, validation_split=0.2,\n",
    "                        epochs=15, batch_size=128,\n",
    "                        callbacks=[keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "                        verbose=1)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d896ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lightfm_model(df):\n",
    "    dataset = Dataset()\n",
    "    dataset.fit(df['user_id'], df['parent_asin'])\n",
    "    interactions, _ = dataset.build_interactions([\n",
    "        (u, i, r) for u, i, r in zip(df['user_id'], df['parent_asin'], df['rating'])\n",
    "    ])\n",
    "    model = LightFM(loss='warp')\n",
    "    model.fit(interactions, epochs=5, num_threads=2)\n",
    "    return model, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae613402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_recommendation_pipeline(category):\n",
    "    train_df, valid_df, test_df = load_and_preprocess_data(category)\n",
    "\n",
    "    all_data = pd.concat([train_df, valid_df])\n",
    "    user_cat = all_data['user_id'].astype('category')\n",
    "    item_cat = all_data['parent_asin'].astype('category')\n",
    "\n",
    "    user_ids = user_cat.cat.codes\n",
    "    item_ids = item_cat.cat.codes\n",
    "    ratings = all_data['rating'].astype(float)\n",
    "\n",
    "    num_users = user_ids.nunique()\n",
    "    num_items = item_ids.nunique()\n",
    "\n",
    "    model_nn, history = train_neural_network(user_ids, item_ids, ratings, num_users, num_items)\n",
    "\n",
    "    val_data = all_data.sample(frac=0.2, random_state=42)\n",
    "    val_user_ids = val_data['user_id'].astype('category').cat.codes\n",
    "    val_item_ids = val_data['parent_asin'].astype('category').cat.codes\n",
    "    val_ratings = val_data['rating'].values\n",
    "\n",
    "    nn_preds = model_nn.predict([val_user_ids, val_item_ids], verbose=0).flatten()\n",
    "    rmse = np.sqrt(mean_squared_error(val_ratings, nn_preds))\n",
    "\n",
    "    model_lfm, dataset = train_lightfm_model(all_data)\n",
    "\n",
    "    sample_user = 0\n",
    "    sample_items = np.arange(num_items)\n",
    "    nn_preds_sample = model_nn.predict([np.full(num_items, sample_user), sample_items], verbose=0).flatten()\n",
    "    lfm_preds_sample = model_lfm.predict(sample_user, sample_items)\n",
    "\n",
    "    top_nn_items = np.argsort(-nn_preds_sample)[:5]\n",
    "    top_lfm_items = np.argsort(-lfm_preds_sample)[:5]\n",
    "\n",
    "    print(f\"Top 5 NN recommendations (item indices): {top_nn_items}\")\n",
    "    print(f\"Top 5 LightFM recommendations (item indices): {top_lfm_items}\")\n",
    "    print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'results': {\n",
    "            'summary': f\"Processed {category} | NN RMSE: {rmse:.4f}\",\n",
    "            'nn_history': history.history,\n",
    "            'nn_rmse': rmse,\n",
    "            'nn_top_items': top_nn_items,\n",
    "            'lfm_top_items': top_lfm_items\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d64eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_all_losses(category_results):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for category, history in category_results.items():\n",
    "        val_loss = history.get('val_loss', [])\n",
    "        if val_loss:\n",
    "            plt.plot(val_loss, label=category.replace(\"_\", \" \"), linewidth=2)\n",
    "    plt.title(\"Validation Loss Across Categories\", fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"Validation Loss\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ee394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = [\n",
    "    \"Office_Products\",\n",
    "    \"Arts_Crafts_and_Sewing\",\n",
    "    \"Baby_Products\",\n",
    "    \"Software\",\n",
    "    \"Video_Games\"\n",
    "]\n",
    "\n",
    "category_results = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\nðŸ“¦ Processing category: {category}\")\n",
    "    result = run_recommendation_pipeline(category)\n",
    "    category_results[category] = result['results']['nn_history']\n",
    "\n",
    "plot_all_losses(category_results)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
